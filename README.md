# Data Val√®ncia Agent (Analista de Datos Abiertos de Valencia)

**Data Val√®ncia Agent** es una aplicaci√≥n web interactiva desarrollada con Streamlit y potenciada por Modelos de Lenguaje Grandes (LLMs) como Gemini y Llama 3. Su objetivo es permitir a cualquier usuario explorar el [cat√°logo de Datos Abiertos del Ayuntamiento de Valencia](https://valencia.opendatasoft.com/pages/home/?flg=es-es) utilizando lenguaje natural.

La aplicaci√≥n encuentra el dataset m√°s relevante para la consulta del usuario, lo descarga, lo analiza y genera visualizaciones y res√∫menes de forma autom√°tica, actuando como un analista de datos virtual.

---

## Caracter√≠sticas Principales

- **B√∫squeda Sem√°ntica**: Utiliza embeddings de sentencias (`sentence-transformers`) y un √≠ndice vectorial (FAISS) para encontrar el dataset m√°s relevante para una consulta en lenguaje natural.
- **Multi-LLM**: Permite cambiar entre diferentes proveedores de LLM (Google Gemini, Llama 3 a trav√©s de Groq) para la planificaci√≥n y generaci√≥n de insights.
- **An√°lisis Autom√°tico de Datos**: Identifica autom√°ticamente tipos de columnas (num√©ricas, categ√≥ricas, geoespaciales, temporales).
- **Generaci√≥n de Visualizaciones**: El LLM planifica y sugiere los gr√°ficos m√°s adecuados (mapas, barras, l√≠neas, etc.) para responder a la consulta del usuario.
- **Creaci√≥n de Insights**: Un agente LLM interpreta los gr√°ficos y los datos para generar un resumen ejecutivo en texto.
- **Interfaz Interactiva**: Construida con Streamlit para una experiencia de usuario fluida y conversacional.

---

## üõ†Ô∏è Arquitectura y Tecnolog√≠as

El proyecto sigue una arquitectura modular basada en agentes, donde cada componente tiene una responsabilidad clara:

- **Frontend**: `Streamlit`
- **B√∫squeda y RAG (Retrieval-Augmented Generation)**:
  - **Embeddings**: `sentence-transformers` (modelo `paraphrase-MiniLM-L6-v2`)
  - **√çndice Vectorial**: `FAISS`
- **Modelos de Lenguaje (LLMs)**:
  - `Google Gemini` (a trav√©s de `google-genai`)
  - `Groq` (a trav√©s de `groq`)
  - `Ollama` (a trav√©s de `ollama`)
- **An√°lisis y Manipulaci√≥n de Datos**: `Pandas`, `NumPy`
- **Visualizaci√≥n**: `Plotly Express`
- **Gesti√≥n de Dependencias**: `uv`
- **Gesti√≥n de Secrets**: `pydantic-settings`

> [!IMPORTANT]
> El proyecto est√° dise√±ado para que solo se pueda usar un proveedor de LLM a la vez.
>
> Para configurar el proveedor, se utilizan variables de entorno

---

## ‚öôÔ∏è Instalaci√≥n y Ejecuci√≥n Local

Sigue estos pasos para ejecutar el proyecto en tu m√°quina local.

### Prerrequisitos

- Python 3.13
- Git

### 1. Clonar el Repositorio

```bash
git clone https://github.com/TU_USUARIO/TU_REPOSITORIO.git
cd TU_REPOSITORIO
```

### 2. Instalar el gestor de dependencias `uv`

`uv` es un gestor de dependencias moderno y r√°pido para Python, escrito en Rust.

Puedes instalarlo con pipx:

```bash
pipx install uv
```

Tambi√©n puedes referirte a la documentaci√≥n oficial de [uv](https://docs.astral.sh/uv) para el instalador oficial standalone.

### 3. Instalar las dependencias

Una vez instalado `uv`, puedes instalar las dependencias del proyecto con:

```bash
uv sync
```

Esto crear√° un entorno virtual `.venv` en la ra√≠z del proyecto e instalar√° todas las dependencias listadas en `pyproject.toml`.

### 4. Configurar las variables de entorno

El proyecto necesita que se configure un proveedor de LLM para funcionar. Actualmente soporta Ollama, Google Gemini y Groq.

Para usar Google Gemini o Groq, necesitas obtener tus claves API de cada servicio. En el caso de Ollama, necesitar proporcionar la URL de tu instancia local (que por defecto es `http://localhost:11434`). Adem√°s, deber√°s configurar el modelo LLM que quieres con ese proveedor.

> [!NOTE]
> Se recomiendan usar los siguientes modelos:
>
> - Ollama: `codestral`
> - Google Gemini: `gemini-1.5-flash-latest`
> - Groq: `llama3-70b-8192`

Para configurar las variables de entorno, copia el archivo `.env.example` a `.env` y agrega los valores que necesites.

#### 4.1 Configuraci√≥n de Ollama (s√≥lo si usas Ollama)

Si quieres usar Ollama, primero necesitas instalarlo y configurar tu modelo localmente. Puedes seguir la gu√≠a oficial de [Ollama](https://ollama.com/docs/installation) para instalarlo. Luego, debes descargar el modelo `codestral` con el siguiente comando:

```bash
ollama pull codestral
```

Despu√©s, en tu archivo `.env`, configura la URL de tu instancia de Ollama (si es diferente a la predeterminada) y el modelo que quieres usar:

```env
LLM_PROVIDER=ollama
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=codestral
```

### 5. Construir el √çndice FAISS (Solo la primera vez)

Para que la b√∫squeda funcione, necesitas crear el √≠ndice vectorial localmente. Para ello est√° el script `build_index.py`, que se encargar√° de descargar la informaci√≥n de los datasets, generar los embeddings y construir el √≠ndice FAISS, que se guardar√° localmente en los archivos `faiss_metadata.json` y `faiss_opendata_valencia.idx` en el directorio `data/`.

Dado que el √≠ndice se construye a partir de la informaci√≥n de datasets y es un contenido relativamente cambiante, no se almacena el √≠ndice en el repositorio, por ello el directorio `data/` est√° incluido en el `.gitignore`. Antes de ejecutar el script, debes crear el directorio `data/` en la ra√≠z del proyecto:

```bash
mkdir data
```

Puedes ejecutar el script con el siguiente comando:

```bash
python build_index.py
```

#### üí° Uso

Escribe una consulta en lenguaje natural en el campo de texto principal (ej: "¬øD√≥nde hay aparcamientos para bicis?").
Haz clic en "Analizar Consulta".
El agente buscar√° el dataset m√°s relevante, lo analizar√° y te presentar√° visualizaciones e insights.
Puedes realizar preguntas de seguimiento sobre el dataset activo.

#### üìà Posibles mejoras futuras

Implementar un sistema de cach√© m√°s avanzado para los resultados de la API.
Permitir al usuario seleccionar manualmente un dataset si la b√∫squeda sem√°ntica no es precisa.
A√±adir soporte para m√°s tipos de visualizaciones.
Mejorar la gesti√≥n de memoria para datasets muy grandes.

### 6. Ejecutar la aplicaci√≥n

¬°Ya est√° todo listo! Inicia la aplicaci√≥n Streamlit con este comando:

```bash
streamlit run
```

La aplicaci√≥n se abrir√° autom√°ticamente en una nueva pesta√±a de tu navegador.

## Despliegue

Para desplegar la aplicaci√≥n se proporciona un `Dockerfile` que puedes usar para crear una imagen Docker de la aplicaci√≥n. Est√° optimizado para producci√≥n, utilizando una imagen base de Python ligera y configurando el entorno de manera eficiente.

Una vez que tengas tu imagen Docker, puedes desplegarla en cualquier plataforma que soporte contenedores. Cuando despliegues la aplicaci√≥n, aseg√∫rate de configurar las variables de entorno necesarias para el proveedor de LLM que hayas elegido y ejecutar el script `build_index.py` para generar el √≠ndice FAISS antes de iniciar la aplicaci√≥n.

## Agradecimientos

- A OpenData Val√®ncia por proporcionar los datos.
- A las comunidades de Streamlit, Hugging Face y FAISS.
- A @vicentcorrecher, creador de [BIaaS](https://github.com/vicentcorrecher/BIaaS), cuyo trabajo fue el punto de partida de esta evoluci√≥n del proyecto.
