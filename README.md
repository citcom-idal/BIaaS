# Data Val√®ncia Agent (Analista de Datos Abiertos de Val√®ncia)

**Data Val√®ncia Agent** es una aplicaci√≥n web interactiva desarrollada con Streamlit y potenciada por Modelos de Lenguaje Grandes (LLMs) como Google Gemini y Llama 3. Su objetivo es permitir a cualquier usuario explorar el [cat√°logo de Datos Abiertos del Ayuntamiento de Val√®ncia](https://valencia.opendatasoft.com/pages/home/?flg=es-es) utilizando lenguaje natural.

La aplicaci√≥n encuentra el dataset m√°s relevante para la consulta del usuario, lo descarga, lo analiza y genera visualizaciones y res√∫menes de forma autom√°tica, actuando como un analista de datos virtual.

---

## Caracter√≠sticas Principales

- **B√∫squeda Sem√°ntica**: Utiliza embeddings de sentencias (`sentence-transformers`) y un √≠ndice vectorial (FAISS) para encontrar el dataset m√°s relevante para una consulta en lenguaje natural.
- **Multi-LLM**: Permite cambiar entre diferentes proveedores de LLM (Google Gemini, Llama 3 a trav√©s de Groq, codestral a trav√©s de Ollama) para la planificaci√≥n y generaci√≥n de insights.
- **An√°lisis Autom√°tico de Datos**: Identifica autom√°ticamente tipos de columnas (num√©ricas, categ√≥ricas, geoespaciales, temporales).
- **Generaci√≥n de Visualizaciones**: El LLM planifica y sugiere los gr√°ficos m√°s adecuados (mapas, barras, l√≠neas, etc.) para responder a la consulta del usuario.
- **Creaci√≥n de Insights**: Un agente LLM interpreta los gr√°ficos y los datos para generar un resumen ejecutivo en texto.
- **Interfaz Interactiva**: Construida con Streamlit para una experiencia de usuario fluida y conversacional.

---

## üõ†Ô∏è Arquitectura y Tecnolog√≠as

El proyecto sigue una arquitectura modular basada en agentes, donde cada componente tiene una responsabilidad clara:

- **Frontend**: `Streamlit`
- **B√∫squeda y RAG (Retrieval-Augmented Generation)**:
  - **Embeddings**: `sentence-transformers` (modelo `paraphrase-MiniLM-L6-v2`)
  - **√çndice Vectorial**: `FAISS`
- **Modelos de Lenguaje (LLMs)**:
  - `Google Gemini` (a trav√©s de `google-genai`)
  - `Groq` (a trav√©s de `groq`)
  - `codestral` (a trav√©s de `ollama`)
- **An√°lisis y Manipulaci√≥n de Datos**: `Pandas`, `NumPy`
- **Visualizaci√≥n**: `Plotly Express`
- **Gesti√≥n de Dependencias**: `uv`
- **Gesti√≥n de Secrets**: `pydantic-settings`

> [!IMPORTANT]
> El proyecto est√° dise√±ado para que solo se pueda usar un proveedor de LLM a la vez.
>
> Para configurar el proveedor, se utilizan variables de entorno.

---

## ‚öôÔ∏è Instalaci√≥n y Ejecuci√≥n Local

Sigue estos pasos para ejecutar el proyecto en tu m√°quina local.

### Prerrequisitos

- Python 3.13
- Git

### 1. Clonar el Repositorio

```bash
git clone https://github.com/TU_USUARIO/TU_REPOSITORIO.git
cd TU_REPOSITORIO
```

### 2. Instalar el gestor de dependencias `uv`

`uv` es un gestor de dependencias moderno y r√°pido para Python, escrito en Rust.

Puedes instalarlo con pipx:

```bash
pipx install uv
```

Tambi√©n puedes consultar la documentaci√≥n oficial de [uv](https://docs.astral.sh/uv) para el instalador standalone.

### 3. Instalar las dependencias

Una vez instalado `uv`, puedes instalar las dependencias del proyecto con:

```bash
uv sync
```

Esto crear√° un entorno virtual `.venv` en la ra√≠z del proyecto e instalar√° todas las dependencias listadas en `pyproject.toml`.

### 4. Configurar las variables de entorno

El proyecto necesita que se configure un proveedor de LLM para funcionar. Actualmente soporta Ollama (local), Google Gemini y Groq.

Para configurar las variables de entorno, copia el archivo `.env.example` a `.env` y agrega los valores que necesites.

Variables clave (nombres actuales que usa la aplicaci√≥n):

- LLM_PROVIDER: Selecciona el proveedor (`ollama`, `gemini`, `groq`).
- LLM_MODEL: Modelo a usar en el proveedor seleccionado.
- LLM_PROVIDER_API_KEY: (Opcional) clave API para proveedores que la requieran (Gemini / Groq).
 - OLLAMA_HOST: URL de la instancia de Ollama si usas Ollama (por defecto `http://localhost:11434`).

Se recomiendan usar los siguientes modelos:

- Ollama: `codestral`
- Google Gemini: `gemini-1.5-flash-latest`
- Groq: `llama3-70b-8192`

#### 4.1 Configuraci√≥n de Ollama (solo si usas Ollama)

Si quieres usar Ollama, primero necesitas instalarlo y configurar tu modelo localmente. Puedes seguir la gu√≠a oficial de [Ollama](https://ollama.com/docs/installation) para instalarlo. A continuaci√≥n, descarga el modelo `codestral` con el siguiente comando:

```bash
ollama pull codestral
```

En tu archivo `.env`, configura las variables, por ejemplo:

```env
LLM_PROVIDER=ollama
LLM_MODEL=codestral
OLLAMA_HOST=http://localhost:11434
```

### 5. Construir el √çndice FAISS (Solo la primera vez)

Para que la b√∫squeda funcione, necesitas crear el √≠ndice vectorial localmente. El script est√° en `scripts/build_index.py` y se encargar√° de descargar la informaci√≥n de los datasets, generar los embeddings y construir el √≠ndice FAISS, que se guardar√° localmente en `data/`.

Antes de ejecutar el script, crea el directorio `data/` en la ra√≠z del proyecto (el contenido no se sube al repositorio):

```bash
mkdir data
```

Ejecuta el script desde la ra√≠z del proyecto (o dentro del contenedor si despliegas en Docker):

```bash
python scripts/build_index.py
```

El proceso generar√° los archivos `faiss_metadata.json` y `faiss_opendata_valencia.idx` dentro de `data/`.

#### üí° Uso

1. Escribe una consulta en lenguaje natural en el campo de texto principal (p. ej.: "¬øD√≥nde hay aparcamientos para bicis?").
2. Haz clic en "Analizar Consulta".
3. El agente buscar√° el dataset m√°s relevante, lo analizar√° y te presentar√° visualizaciones e insights.
4. Puedes realizar preguntas de seguimiento sobre el dataset activo.

#### üìà Posibles mejoras futuras

- Implementar un sistema de cach√© m√°s avanzado para los resultados de la API.
- Permitir al usuario seleccionar manualmente un dataset si la b√∫squeda sem√°ntica no es precisa.
- A√±adir soporte para m√°s tipos de visualizaciones.
- Mejorar la gesti√≥n de memoria para datasets muy grandes.

### 6. Ejecutar la aplicaci√≥n

¬°Ya est√° todo listo! Inicia la aplicaci√≥n Streamlit con este comando desde la ra√≠z del proyecto:

```bash
streamlit run streamlit_app.py
```

La aplicaci√≥n se abrir√° autom√°ticamente en una nueva pesta√±a de tu navegador.

## Despliegue

Para desplegar la aplicaci√≥n se proporciona un `Dockerfile` que puedes usar para crear una imagen Docker de la aplicaci√≥n. Est√° optimizado para producci√≥n, utilizando una imagen base de Python ligera y configurando el entorno de manera eficiente.

Cuando despliegues la aplicaci√≥n, aseg√∫rate de configurar las variables de entorno necesarias para el proveedor de LLM que hayas elegido y de ejecutar el script `scripts/build_index.py` desde el contenedor para generar el √≠ndice FAISS antes de iniciar la aplicaci√≥n.

## Agradecimientos

- A OpenData Val√®ncia por proporcionar los datos.
- A las comunidades de Streamlit, Hugging Face y FAISS.
- A @vicentcorrecher, creador de [BIaaS](https://github.com/vicentcorrecher/BIaaS), cuyo trabajo fue el punto de partida de esta evoluci√≥n del proyecto.
