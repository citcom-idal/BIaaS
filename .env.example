# Configure your LLM provider here (ollama, gemini, groq)
LLM_PROVIDER=

# Configure your LLM model here. The recommended models for each provider are:
# - Ollama: codestral
# - Google Gemini: gemini-1.5-flash-latest
# - Groq: llama3-70b-8192
LLM_MODEL=

# Specify the API key for your LLM provider if required. Leave empty if not needed.
LLM_PROVIDER_API_KEY=

# For Ollama users:
# Specify the host URL for your Ollama instance.
# Default is http://localhost:11434.
# Adjust if your Ollama instance is hosted elsewhere.
OLLAMA_HOST=

# Enable or disable caching for embedding models. Set to true to enable caching, false to disable.
# Caching allow to use prevously downloaded models without trying to download them.
USE_EMBEDDING_MODEL_CACHE=