# Configure your LLM provider here (ollama, gemini, groq)
LLM_PROVIDER=

# Configure your LLM model here (e.g., for ollama: llama3:8b). Leave empty to use the provider's default model.
LLM_MODEL=

# Specify the API key for your LLM provider if required. Leave empty if not needed.
LLM_PROVIDER_API_KEY=

# For Ollama users:
# Specify the host URL for your Ollama instance.
# Default is http://localhost:11434.
# Adjust if your Ollama instance is hosted elsewhere.
OLLAMA_HOST=